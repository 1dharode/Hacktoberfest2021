from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from matplotlib import pyplot as plt

sentences = [['human', 'interface', 'computer'],
            ['survey', 'user', 'computer', 'system', 'response', 'time'],
            ['eps', 'user', 'interface', 'system'],
            ['system', 'human', 'system', 'eps'],
            ['user', 'response', 'time'],
            ['trees'],
            ['graph', 'trees'],
            ['graph', 'minors', 'trees'],
            ['graph', 'minors', 'survey']]

model = Word2Vec(sentences = sentences, min_count = 1, vector_size = 10)

print(model.wv['user'])

vectors = [ model.wv[word] for word in model.wv.index_to_key]

[ [word,model.wv[word]] for word in model.wv.index_to_key]

pca = PCA(n_components = 2)
result = pca.fit_transform(vectors)

result

plt.scatter(result[:, 0], result[:, 1])
words = model.wv.index_to_key

for i,word in enumerate(words):
  plt.annotate(word,(result[i,0],result[i,1]))
  print(i,word)


words = model.wv.index_to_key

for word in words:
  print(word)

